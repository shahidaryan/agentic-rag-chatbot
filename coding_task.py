# -*- coding: utf-8 -*-
"""Coding Task

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r6YoPD-VMgk7QYKGoCTgKHKSfmTLVTdX
"""

!pip install -q faiss-cpu sentence-transformers python-docx python-pptx PyMuPDF pandas ipywidgets transformers

import io
import faiss
import fitz
import pandas as pd
import docx
from pptx import Presentation
import numpy as np
import re
from sentence_transformers import SentenceTransformer
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import ipywidgets as widgets
from IPython.display import display
import uuid

class MCPMessage:
    def __init__(self, sender, receiver, type_, payload, trace_id=None):
        self.message = {
            "sender": sender,
            "receiver": receiver,
            "type": type_,
            "trace_id": trace_id or str(uuid.uuid4()),
            "payload": payload
        }

    def get(self):
        return self.message

class IngestionAgent:
    @staticmethod
    def split_text(text, max_chunk_length=500):
        sentences = re.split(r'(?<=[.!?]) +', text)
        chunks, current = [], ""
        for sentence in sentences:
            if len(current) + len(sentence) <= max_chunk_length:
                current += sentence + " "
            else:
                chunks.append(current.strip())
                current = sentence + " "
        if current:
            chunks.append(current.strip())
        return chunks

    @staticmethod
    def parse_file(file_obj):
        name = file_obj.name.lower()
        content = []

        if name.endswith(".pdf"):
            doc = fitz.open(stream=file_obj.read(), filetype="pdf")
            for page in doc:
                content.append(page.get_text())
        elif name.endswith(".docx"):
            d = docx.Document(file_obj)
            content.append("\n".join(p.text for p in d.paragraphs))
        elif name.endswith(".pptx"):
            prs = Presentation(file_obj)
            slides = []
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        slides.append(shape.text)
            content.append("\n".join(slides))
        elif name.endswith(".csv"):
            df = pd.read_csv(file_obj)
            content.append(df.to_string())
        elif name.endswith(".txt") or name.endswith(".md"):
            content.append(file_obj.read().decode())
        else:
            content.append("Unsupported file format.")

        all_text = "\n".join(content)
        return [(i, chunk) for i, chunk in enumerate(IngestionAgent.split_text(all_text))]

class RetrievalAgent:
    def __init__(self):
        self.model = SentenceTransformer("all-MiniLM-L6-v2")
        self.index = None
        self.indexed_chunks = []

    def build_index(self, indexed_chunks):

        self.indexed_chunks = indexed_chunks
        chunk_texts = [chunk for (_, chunk) in indexed_chunks]
        embeddings = self.model.encode(chunk_texts)
        dim = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dim)
        self.index.add(np.array(embeddings))

    def retrieve(self, query, k=5):
        query_vec = self.model.encode([query])
        _, I = self.index.search(np.array(query_vec), k)

        return [self.indexed_chunks[i] for i in I[0]]

class LLMResponseAgent:
    @staticmethod
    def generate(mcp_msg):
        unique_chunks = list(set(mcp_msg["payload"]["retrieved_context"]))
        context = "\n".join(unique_chunks)
        query = mcp_msg["payload"]["query"]

        prompt = (
    "You are an expert assistant. The following context includes multiple machine learning topics.\n"
    "For the given question, extract and summarize all relevant concepts in a clean, organized, and concise answer.\n"
    "If multiple algorithms or terms are present, list and explain them clearly without repeating text.\n\n"
    f"Context:\n{context}\n\nQuestion:\n{query}\n\nAnswer:"
)


        try:
            print("ðŸš€ Sending prompt to model...")
            result = hf_pipe(prompt, max_new_tokens=256, do_sample=False)[0]['generated_text']
            print("âœ… LLM responded.")
        except Exception as e:
            print("âŒ Hugging Face model error:", e)
            return {
                "answer": "LLM generation failed due to an error.",
                "sources": []
            }

        answer = result.split("Answer:")[-1].strip()
        return {
            "answer": answer,
            "sources": unique_chunks
        }

upload_widget = widgets.FileUpload(accept='.pdf,.docx,.pptx,.csv,.txt,.md', multiple=True)
query_input = widgets.Textarea(placeholder="Ask a question...", layout=widgets.Layout(width='600px', height='60px'))
submit_button = widgets.Button(description="Submit", button_style='success')
output = widgets.Output()

display(widgets.VBox([upload_widget, query_input, submit_button, output]))
retriever = RetrievalAgent()

def handle_click(_):
    output.clear_output()
    with output:
        print("âœ… Button clicked!")

        if not upload_widget.value:
            print("â— Please upload at least one file.")
            return

        query = query_input.value.strip()
        if not query:
            print("â— Please enter a question.")
            return

        indexed_chunks = []  # (index, chunk_text)
        for f in upload_widget.value.values():
            file_obj = io.BytesIO(f["content"])
            file_obj.name = f["metadata"]["name"]
            file_chunks = IngestionAgent.parse_file(file_obj)  # returns (index, text)
            indexed_chunks.extend(file_chunks)

        print(f"ðŸ“„ Parsed {len(indexed_chunks)} chunks from {len(upload_widget.value)} file(s).")

        retriever.build_index(indexed_chunks)

        retrieved_pairs = retriever.retrieve(query, k=5)

        sorted_chunks = [text for (idx, text) in sorted(retrieved_pairs, key=lambda x: x[0])]

        mcp_msg = MCPMessage(
            sender="RetrievalAgent",
            receiver="LLMResponseAgent",
            type_="RETRIEVAL_RESULT",
            payload={"retrieved_context": sorted_chunks, "query": query}
        ).get()

        print("ðŸ¤– Generating answer...")
        result = LLMResponseAgent.generate(mcp_msg)

        print("\nðŸ“Œ Answer:\n", result["answer"])
        print("\nðŸ“š Source Chunks:")
        for i, chunk in enumerate(result["sources"]):
            print(f"\nðŸ”¹ Chunk {i+1}:\n{chunk[:300]}...")  






submit_button.on_click(handle_click)
âœ… Button clicked!
ðŸ“„ Parsed 113 chunks from 1 file(s).
ðŸ¤– Generating answer...
ðŸš€ Sending prompt to model...
âœ… LLM responded.

ðŸ“Œ Answer:
 Overfitting is a type of modeling error that results in the failure to predict or guess the future observations effectively or fit additional data in the model that already exists.

ðŸ“š Source Chunks:

ðŸ”¹ Chunk 1:
The next time an email hits the inbox, the spam filter will use statistical analysis and
algorithms like Decision Trees and SVM to identify how likely the email is spam.
5. If the probability is high, then it will be labeled as spam, and the email will not hit your
inbox.
6. Based on the accuracy of...

ðŸ”¹ Chunk 2:
What Is Overfitting?
Overfitting is a type of modeling error that results in the failure to predict or guess the future
observations effectively or fit additional data in the model that already exists.
24. Explain The Terms Standard Deviation And Variance?
A standard deviation is defined as the numb...

ðŸ”¹ Chunk 3:
Can You Explain The OOB Error?
An out-of-bag error called OBB error, also known as an out-of-bag estimate, is a technique to
measure the prediction error of random forests, boosted decision trees. Bagging mainly uses
subsampling with replacement to create the training samples for the model to learn ...

ðŸ”¹ Chunk 4:
Cross-validation: The idea here is to use the initial training data to generate various
small train test spills. Where these test spills are used to tune the model.
2. Train with more data: Training with a lot of data can help the algorithms to detect the
signals better.
3. Remove feature: You can m...

ðŸ”¹ Chunk 5:
Explain Pruning In Decision Trees, And How Is It Done?
Pruning is a data compression process in machine learning and search algorithms that can
reduce the size of the decision trees by removing certain sections of the tree that are non-critical
and unnecessary to classify instances. A tree that is t...
